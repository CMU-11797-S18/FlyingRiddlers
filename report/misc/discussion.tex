\begin{table*}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|} 
    \hline \hline
    Model & Exact Answers & Exact Answers & Exact Answers & Ideal Answers \\
    &Yes/No type& Factoid  type& List type & All types \\
    & Accuracy (\%) & MRR & F1 score & ROUGE-2\\
    \hline \hline
    \cite{khyati-paper} & - & - & - & 0.653  \\
    \hline
    \cite{fudan}&\textbf{0.714}&0.272& 0.187& -\\
    \hline
    \cite{fastqa}& - &\textbf{0.392}& \textbf{0.361}&-\\
    \hline
    Sarrouti and Alaoui \shortcite{usmba}&0.461&0.207&0.243&0.577\\
    \hline
    \textit{BioAMA}(Ours)&0.653& 0.195&0.234&\textbf{0.721}\\
    \hline \hline
    
    \end{tabular}
    \caption{Comparison of our question-type based QA model with other state of the art approaches on BioASQ 5b dataset}
    \label{tab:comparison_results}
\end{table*}

\section{Discussion}

In this section, we shall analyze our gross findings from building the two QA systems and experimenting on both the datasets. Having built a broad set of classifiers and modules, we also comment on our experiences in building these systems while highlighting our key learning from these experiences. 

\subsection{Model Comparison}

The question-type based QA system that we built performs remarkably well on the BioASQ. This can be noted from the Table \ref{tab:comparison_results}, where we compare our system with the state of the art systems on BioASQ. We can see that our system performs the best on Ideal Answers and is very close to the best performances on Exact Answers. The trends are very similar for BioASQ 6b as well, indicating that a question-type based system is very effective for BioASQ. However, our end-to-end system fails to perform well on the BioASQ, as shown in Table \ref{tab:bioasq_res}, which leads us to the conclusion that for smaller datasets, a question-type based modular system is typically a much better choice of QA system.

On the other hand, we note that the end-to-end systems that we proposed has much better performances on the large datasets such as MS MARCO and SQuAD at the ranks of some of the state-of-the-art systems, while our modular question-type based system fails to yield to similar performance. This should not be surprising, since our end-to-end QA system is heavily parametrized and a lot more capable, and having a large dataset would help us efficiently train the model. In the cases of smaller datasets, however, we would have to aid the model with inductive biases, and constrain it with very few parameters to strike a trade off between efficient training and optimal performance. 

With these findings, we reject our general hypothesis that a holistic QA model can be trained to yield consistently high performances across multiple datasets, and conclude that a well performing QA system would have to be carefully chosen with the size and domain of the dataset as key considerations.

\subsection{Lessons Learned}

Building the two QA systems on two separate datasets involved a lot of design and implementation choices some of which, in hindsight, were sub-optimal. However, these choices coupled with our experimental findings have led us to have many insightful realizations, some of which are:

\begin{itemize}

    \item Simple approaches can often yield more reliable and strong results than complicated ones. We realized that this is applicable to different question types:
    \begin{itemize}
        \item In case of yes/no questions, a simple linear transformation for embedding worked much better than a neural network-based non-linear transformation for augmenting word embeddings
        \item In the case of factoid/list questions, having n-gram based candidates worked much better than having NER-based candidates
        % \item The most important features might seem very trivial but works very well for ( Indri, BM25 etc.) 
    \end{itemize}
    \item A long and in-depth dataset exploration is key to building features, especially for small datasets. However, it is important to not rely on specific aspects of a subset of the data as it is easy to overfit if the features come from a biased subset of the data. We learned this lesson when analyzing the data factoid-list type questions led us to have better features and candidates
    \item Additional computation time should be weighed against marginal performance gain. We learned this when we realized that some NER extraction techniques were far more expensive to compute than the marginal value they added to the ranking performance. This also applied to the choice of LeToR ranking algorithms where it was not feasible to efficiently fine-tune pairwise algorithms like SVMRank with a large number of answer candidates.
    \item Models for small datasets can be extremely sensitive of feature weighting and selection. 
    % \item Unsupervised approaches 
    % \item Ranking (LeToR) approaches are strongly tied to computational power (after a certain point, pairwise is not realistic).
    % \item Entailment-based tasks show high transfer learning potential
    % Since the yes/no answer is computed based on gross entailment statistics, the answers are less sensitive to errors in entailment
    % \item An advantage of an unsupervised classifier is that class imbalances are handled well
    % \item 
    \item Explicit Diversification methods need to be adapted to remove redundancy in the summary generated. This is applicable to the description category questions for any given type of dataset be it a large dataset like MS MARCO or a smaller dataset like BioASQ. The boost in ROUGE score and BLEU score we got from adapting Maximum Marginal Relevance is a strong indicator of the same.
    \item Even for datasets of large sizes, the QA system would depend heavily on the specific domain of the dataset. For example, PoS based features perform better for question type classification in Web search compared to BioMedical domain.
% \item Inverse relation between word limit and Rouge
% \item Dice coefficient works a tad bit better  with Indri as both are language model based.
% \item If the answer is present in single snippet, narrowing down that snippet is hard (MS Marco) 
% \item For Web search type queries, rules (PoS based) work surprisingly well as initial filter.
% \item Summary of BioASQ and Description of MS Marco are not exactly the same. Summary is a subset of Description.

\end{itemize}

\section{Individual Contributions}

By virtue of having a tightly integrated system that was built collaboratively, we have had to work very closely with each other on almost all the modules involved. Though we all weighed in for the design and analysis of the systems, each one of us also assumed primary responsibility for implementation of specific parts of the system, which have been tabulated in Table \ref{tab:contributions}.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c|c|c} \hline
    Team Member & Yes/No Type & Factoid/List Type & Ideal Answers & End-to-end System \\ \hline \hline
    Nitish & \checkmark & \checkmark & & \\
    Gabriel & & \checkmark & \checkmark & \\
    Pranavi & & \checkmark & \checkmark & \\
    Vasu & \checkmark & & & \checkmark \\ \hline
    \end{tabular}
    \caption{Key contributions from individual members of the team}
    \label{tab:contributions}
\end{table}

% Our key novel contributions in this paper are:
% \begin{enumerate}
%     \item \textbf{State of the art result on ideal answer questions}: Our proposed \textit{BioAMA} system attains state of the art results in automatic evaluation measures for the ideal answer questions in Task 5b of the BioASQ dataset, yielding a 7\%  improvement over the previous state of the art system \cite{khyati-paper}.
%     \item \textbf{Novel framework for Yes/No type QA}: 
%     We introduce a novel NLI-based approach for answering the Yes/No style questions in the BioASQ dataset. We model these questions as a Textual Entailment (TE) problem and use Hierarchical Convolutional Neural Network based Infersent models \cite{Infersent} to answer this type of question. To address the challenge of inadequate training data, we also introduce a novel embedding projection technique which allows for effective transfer learning from models trained on larger datasets with a different vocabulary to work on the much smaller BioASQ dataset.
%     \item \textbf{A novel two-stage approach to answer Factoid and List type questions}: 
%     By using an ensemble of biomedical NER taggers to generate a candidate answer set, we devise unsupervised and supervised ranking algorithms to generate the final predictions.
%     \item \textbf{Improved Maximum Marginal Relevance (MMR) framework}: We improve upon the MMR framework for relevant sentence selection from the chosen snippets. The use of MMR for sentence selection was introduced in the work of Chandu et al. \shortcite{khyati-paper}. We improve upon this framework by experimenting with a number of more informative similarity metrics to replace and improve upon the baseline Jaccard similarity metric.

% \end{enumerate}
