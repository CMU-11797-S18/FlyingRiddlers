
% \begin{abstract}
% In this paper, we present a novel Biomedical Question Answering system, \textit{BioAMA}: ``Biomedical Ask Me Anything" on task 5b of the annual BioASQ challenge \cite{bioasq}. In this work, we focus on a wide variety of question types including factoid, list based, summary and yes/no type questions that generate both exact and well-formed `ideal' answers. For summary-type questions, we combine effective IR-based techniques for retrieval and diversification of relevant snippets for a question to create an end-to-end system which achieves a ROUGE-2 score of 0.72 and a ROUGE-SU4 score of 0.71 on ideal answer questions (7\%  improvement over the previous best model). Additionally, we propose a novel NLI-based framework to answer the yes/no questions. To train the NLI model, we also devise a transfer-learning technique by cross-domain projection of word embeddings. Finally, we present a two-stage approach to address the factoid and list type questions by first generating a candidate set using NER taggers and ranking them using both supervised or unsupervised techniques.

% \end{abstract}

\section{Introduction}

% In the era of ever advancing medical sciences and the age of the internet, a remarkable amount of medical literature is constantly being posted online. This has led to a need for an effective retrieval and indexing system which can allow us to extract meaningful information from these vast knowledge sources. One of the most effective and natural ways to leverage this huge amount of data in real life is to build a Question Answering system which will allow us to directly query this data and extract meaningful and structured information in a human readable form. 

In the last few years, there have been tremendous advancements in the field of Question Answering, with the neural Question Answering systems even outperforming humans in some cases[\cite{msmarco}]. However, there is also a large diversity in the type and nature of QA systems, and most systems are very specific to a certain dataset. For a new QA dataset, it is still hard to predict what kind of QA models would be most befitting and the extent to which a QA model trained on one dataset would work on another.  To that end, we present a comprehensive investigation into different types of QA systems based on question and answer types, as well as analyze the merits and demerits of having a modularized question-type based system as opposed to an end-to-end QA system. 

\subsection{General Hypothesis}

With the key idea to assess the generalizability of a Question Answering system, we formulate our general hypothesis as: \\

For the task of Question Answering (QA), a comprehensive and holistic system can be designed that can handle multiple question/answer types and can yield a good performance across a number of QA datasets.

\subsection{Scope}

In order to test our hypothesis, we design and implement broadly two types of Question Answering systems - 1) a modularised question-type specific system and 2) an end-to-end system. We test the performance of these systems on two different datasets (one small and one large) in order to identify a general pattern of relative performances. 

We confine the scope of this work to the question types - yes/no, factoid, list, summary and description. We also consider fixed-sized manually-curated datasets in order to build supervised QA systems. The key focus of our work is, as mentioned earlier, to compare the relative performances of the two systems across different datasets and it is not to develop a single system that works best on a specific dataset. That being said, we do employ several techniques to improve the individual performances of both the systems in order to ensure a fair comparison.

% In this paper, we present  an end-to-end question answering system designed to handle the large variety of question types present in the BioASQ dataset which we call as ``\textit{BioAMA}: Biomedical Ask Me Anything".   \\

    %\item \textbf{Incorporating Information retrieval in the Question Answering pipeline}:
    %We incorporate Indri and BM25 based features in our pipeline to allow for effective information retrieval from the biomedical knowledgebase which allows us to choose the snippets which are most relevant to a given question. 
    %\item \textbf{Statistical ranking models}: We use the powerful LeTOR framework to allow for ranking of our snippets chosen by the information retrieval mechanism. This allows us to maximize the information we extract from the snippets which are deemed more relevant to a specific question. This also blends in well with our MMR framework which makes use of the ranking framework to extract the most relevant and non-repetitive information from the chosen snippets.
    
    %The rest of the paper is organized as follows. Section \ref{lit} presents the prior work on this problem followed by details of the BioASQ challenge and dataset in Section \ref{Dataset}. We present our approach and results for the ideal answer type questions in Section \ref{approach1} and for the exact answer type generation in Section \ref{approach2}. Finally we present the conclusions and future directions of our work in Section \ref{future}.
