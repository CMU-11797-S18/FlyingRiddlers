
\section{Conclusion and Future Work}
\label{future}

In this work, we propose two novel QA systems and present a comprehensive analysis of relative performances the systems two standard QA datasets (BioASQ and MS MARCO) with a primary aim to assess the feasibility of having a unified well-performing model across multiple datasets. Our key finding is that a modularized QA system involving question-type specific models performs quite well on smaller datasets (such as BioASQ) in comparison to the larger datasets. We also find that, while such systems can have similar performances on larger datasets as well, end-to-end systems can perform much better on these since they can leverage the large sizes of the datasets to a much greater extent. 

To achieve state-of-the-art performances on BioASQ, we present an integrated framework for tackling both ideal and exact answer type questions and obtain state of the art results on the ideal answer type questions on the BioASQ dataset. In our framework for exact answers, we incorporate neural entailment models along with a novel embedding transformation technique for answering yes/no questions, and employ LeToR ranking models to answer factoid/list based questions. For ideal answers, we aimed at improving the Information Retrieval component of the extractive summarization. Although this improved ROUGE scores considerably, the human readability aspect of the generated summary answer was not improved to a great extent. We also build an end-to-end system with enhancements to DCN model as well as a free-form answer generation model, which achieves performances close to state-of-the-art systems for SQuAD and MS MARCO datasets.

As future directions, we believe that abstractive summarization based approaches like Pointer Generator Networks \cite{PGC} and Reinforcement Learning based abstractive summarization techniques \cite{salesforce} can greatly improve the human readability of ideal answers for BioASQ dataset. However, to accomplish this, it is elemental to first identify more suitable evaluation metrics that quantify human readability of the ideal answers. For the yes/no answers, there is a lot of scope in using the entailment scores for for improving the classification scores. One such way is to build a supervised classifier that also accounts for the class bias in the dataset. It would also be helpful to heuristically create evaluation metrics for each module in the yes/no pipeline so as to individually improve the performance of each sub-module. Lastly, for the factoid/list/entity type answers, we believe that there is a great potential in exploring more candidate spaces and other LeToR ranking algorithms that also use NER-based features. 

Despite the increasing progress in area of developing well-performing Question Answering systems, there is still a large scope of improvement for the existing models. We believe that our work helps better understand the nuances the existing QA systems and serves as a valuable step towards the larger quest for building intelligent QA systems for real-life use cases.

